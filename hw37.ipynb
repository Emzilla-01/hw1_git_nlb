{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-dryer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unable-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    print('no sc to stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "burning-interview",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"Emy Kay \n",
    "Pyspark \n",
    "Assignment 37\n",
    "\n",
    "1. word count\n",
    "Data Description:\n",
    "LoanStats_2018Q4.csv\n",
    "homeOwnership - The home ownership status provided by the borrower during registration. Our values are: RENT, OWN, MORTGAGE, OTHER.\n",
    "grade - LC assigned loan grade\n",
    "purpose - A category provided by the borrower for the loan request.\n",
    "intRate - Interest Rate on the loan\n",
    "addrState - The state provided by the borrower in the loan application\n",
    "loan_status - Current status of the loan\n",
    "application_type - Indicates whether the loan is an individual application or a joint application with two co-borrowers\n",
    "loan_amnt - The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.\n",
    "emp_length - Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.\n",
    "annual_inc - The self-reported annual income provided by the borrower during registration.\n",
    "dti - A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.\n",
    "dti_joint - A ratio calculated using the co-borrowers' total monthly payments on the total debt obligations, excluding mortgages and the requested LC loan, divided by the co-borrowers' combined self-reported monthly income\n",
    "delinq_2yrs - The number of 30+ days past-due incidences of delinquency in the borrower's credit file for the past 2 years\n",
    "revol_util - Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.\n",
    "total_acc - The total number of credit lines currently in the borrower's credit file\n",
    "num_tl_90g_dpd_24m - Number of accounts 90 or more days past due in last 24 months\n",
    "\n",
    "\n",
    "Analyse the Load Data\n",
    "a) Create a Hive Table using Pyspark\n",
    "\n",
    "\"\"\"\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "assured-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_data_path=r'/home/hadoop/Downloads/LoanStats_2018Q4.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-metro",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "emerging-school",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>wordcount0</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[4] appName=wordcount0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName('wordcount0').setMaster('local[4]')\n",
    "sc = SparkContext.getOrCreate(conf = conf)\n",
    "\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType\n",
    "\n",
    "spark = SparkSession.builder.appName('wordcount0').getOrCreate()\n",
    "sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-discount",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "continuing-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hive integration with spark session\n",
    "    \n",
    "spark = (SparkSession.builder.appName('pysparkhiveintegration')\n",
    "        .config('spark.sql.warehouse.dir', '/user/hive/warehouse')\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "stopped-providence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.types as st\n",
    "loans_data_path=r'/home/hadoop/Downloads/LoanStats_2018Q4.csv'\n",
    "\n",
    "loans_df = spark.read.csv(\"file://{}\".format(loans_data_path), inferSchema=True, header=True)\n",
    "\n",
    "\n",
    "# schema = StructType([\n",
    "#     StructField('UserId', IntegerType()),\n",
    "#     StructField('movieId', IntegerType()),\n",
    "#     StructField('rating', DoubleType()),\n",
    "#     StructField('timestamp', IntegerType()) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "needed-requirement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'manually removed \"Notes offered by Prospectus (https://www.lendingclub.com/info/prospectus.action)\" from csv file'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display('manually removed \"Notes offered by Prospectus (https://www.lendingclub.com/info/prospectus.action)\" from csv file')\n",
    "\n",
    "#display( 'other alternative may be comment=\"N\" ??')\n",
    "#loans_df = spark.read.csv(\"file://{}\".format(loans_data_path), inferSchema=True, header=True, comment=\"N\") #??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "intellectual-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loans_df = spark.read.csv(\"file://{}\".format(loans_data_path), inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "marked-colony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(id,StringType,true),StructField(member_id,StringType,true),StructField(loan_amnt,IntegerType,true),StructField(funded_amnt,IntegerType,true),StructField(funded_amnt_inv,DoubleType,true),StructField(term,StringType,true),StructField(int_rate,StringType,true),StructField(installment,DoubleType,true),StructField(grade,StringType,true),StructField(sub_grade,StringType,true),StructField(emp_title,StringType,true),StructField(emp_length,StringType,true),StructField(home_ownership,StringType,true),StructField(annual_inc,DoubleType,true),StructField(verification_status,StringType,true),StructField(issue_d,StringType,true),StructField(loan_status,StringType,true),StructField(pymnt_plan,StringType,true),StructField(url,StringType,true),StructField(desc,StringType,true),StructField(purpose,StringType,true),StructField(title,StringType,true),StructField(zip_code,StringType,true),StructField(addr_state,StringType,true),StructField(dti,DoubleType,true),StructField(delinq_2yrs,IntegerType,true),StructField(earliest_cr_line,StringType,true),StructField(inq_last_6mths,IntegerType,true),StructField(mths_since_last_delinq,IntegerType,true),StructField(mths_since_last_record,IntegerType,true),StructField(open_acc,IntegerType,true),StructField(pub_rec,IntegerType,true),StructField(revol_bal,IntegerType,true),StructField(revol_util,StringType,true),StructField(total_acc,IntegerType,true),StructField(initial_list_status,StringType,true),StructField(out_prncp,DoubleType,true),StructField(out_prncp_inv,DoubleType,true),StructField(total_pymnt,DoubleType,true),StructField(total_pymnt_inv,DoubleType,true),StructField(total_rec_prncp,DoubleType,true),StructField(total_rec_int,DoubleType,true),StructField(total_rec_late_fee,DoubleType,true),StructField(recoveries,DoubleType,true),StructField(collection_recovery_fee,DoubleType,true),StructField(last_pymnt_d,StringType,true),StructField(last_pymnt_amnt,DoubleType,true),StructField(next_pymnt_d,StringType,true),StructField(last_credit_pull_d,StringType,true),StructField(collections_12_mths_ex_med,IntegerType,true),StructField(mths_since_last_major_derog,IntegerType,true),StructField(policy_code,IntegerType,true),StructField(application_type,StringType,true),StructField(annual_inc_joint,DoubleType,true),StructField(dti_joint,DoubleType,true),StructField(verification_status_joint,StringType,true),StructField(acc_now_delinq,IntegerType,true),StructField(tot_coll_amt,IntegerType,true),StructField(tot_cur_bal,IntegerType,true),StructField(open_acc_6m,IntegerType,true),StructField(open_act_il,IntegerType,true),StructField(open_il_12m,IntegerType,true),StructField(open_il_24m,IntegerType,true),StructField(mths_since_rcnt_il,IntegerType,true),StructField(total_bal_il,IntegerType,true),StructField(il_util,IntegerType,true),StructField(open_rv_12m,IntegerType,true),StructField(open_rv_24m,IntegerType,true),StructField(max_bal_bc,IntegerType,true),StructField(all_util,IntegerType,true),StructField(total_rev_hi_lim,IntegerType,true),StructField(inq_fi,IntegerType,true),StructField(total_cu_tl,IntegerType,true),StructField(inq_last_12m,IntegerType,true),StructField(acc_open_past_24mths,IntegerType,true),StructField(avg_cur_bal,IntegerType,true),StructField(bc_open_to_buy,IntegerType,true),StructField(bc_util,DoubleType,true),StructField(chargeoff_within_12_mths,IntegerType,true),StructField(delinq_amnt,IntegerType,true),StructField(mo_sin_old_il_acct,IntegerType,true),StructField(mo_sin_old_rev_tl_op,IntegerType,true),StructField(mo_sin_rcnt_rev_tl_op,IntegerType,true),StructField(mo_sin_rcnt_tl,IntegerType,true),StructField(mort_acc,IntegerType,true),StructField(mths_since_recent_bc,IntegerType,true),StructField(mths_since_recent_bc_dlq,IntegerType,true),StructField(mths_since_recent_inq,IntegerType,true),StructField(mths_since_recent_revol_delinq,IntegerType,true),StructField(num_accts_ever_120_pd,IntegerType,true),StructField(num_actv_bc_tl,IntegerType,true),StructField(num_actv_rev_tl,IntegerType,true),StructField(num_bc_sats,IntegerType,true),StructField(num_bc_tl,IntegerType,true),StructField(num_il_tl,IntegerType,true),StructField(num_op_rev_tl,IntegerType,true),StructField(num_rev_accts,IntegerType,true),StructField(num_rev_tl_bal_gt_0,IntegerType,true),StructField(num_sats,IntegerType,true),StructField(num_tl_120dpd_2m,IntegerType,true),StructField(num_tl_30dpd,IntegerType,true),StructField(num_tl_90g_dpd_24m,IntegerType,true),StructField(num_tl_op_past_12m,IntegerType,true),StructField(pct_tl_nvr_dlq,DoubleType,true),StructField(percent_bc_gt_75,DoubleType,true),StructField(pub_rec_bankruptcies,IntegerType,true),StructField(tax_liens,IntegerType,true),StructField(tot_hi_cred_lim,IntegerType,true),StructField(total_bal_ex_mort,IntegerType,true),StructField(total_bc_limit,IntegerType,true),StructField(total_il_high_credit_limit,IntegerType,true),StructField(revol_bal_joint,IntegerType,true),StructField(sec_app_earliest_cr_line,StringType,true),StructField(sec_app_inq_last_6mths,IntegerType,true),StructField(sec_app_mort_acc,IntegerType,true),StructField(sec_app_open_acc,IntegerType,true),StructField(sec_app_revol_util,DoubleType,true),StructField(sec_app_open_act_il,IntegerType,true),StructField(sec_app_num_rev_accts,IntegerType,true),StructField(sec_app_chargeoff_within_12_mths,IntegerType,true),StructField(sec_app_collections_12_mths_ex_med,IntegerType,true),StructField(sec_app_mths_since_last_major_derog,IntegerType,true),StructField(hardship_flag,StringType,true),StructField(hardship_type,StringType,true),StructField(hardship_reason,StringType,true),StructField(hardship_status,StringType,true),StructField(deferral_term,IntegerType,true),StructField(hardship_amount,DoubleType,true),StructField(hardship_start_date,StringType,true),StructField(hardship_end_date,StringType,true),StructField(payment_plan_start_date,StringType,true),StructField(hardship_length,IntegerType,true),StructField(hardship_dpd,IntegerType,true),StructField(hardship_loan_status,StringType,true),StructField(orig_projected_additional_accrued_interest,DoubleType,true),StructField(hardship_payoff_balance_amount,DoubleType,true),StructField(hardship_last_payment_amount,DoubleType,true),StructField(debt_settlement_flag,StringType,true),StructField(debt_settlement_flag_date,StringType,true),StructField(settlement_status,StringType,true),StructField(settlement_date,StringType,true),StructField(settlement_amount,IntegerType,true),StructField(settlement_percentage,DoubleType,true),StructField(settlement_term,IntegerType,true)))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "extreme-serial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_needConversion',\n",
       " '_needSerializeAnyField',\n",
       " 'add',\n",
       " 'fieldNames',\n",
       " 'fields',\n",
       " 'fromInternal',\n",
       " 'fromJson',\n",
       " 'json',\n",
       " 'jsonValue',\n",
       " 'names',\n",
       " 'needConversion',\n",
       " 'simpleString',\n",
       " 'toInternal',\n",
       " 'typeName']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(loans_df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "lightweight-package",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField(id,StringType,true),\n",
       " StructField(member_id,StringType,true),\n",
       " StructField(loan_amnt,IntegerType,true),\n",
       " StructField(funded_amnt,IntegerType,true),\n",
       " StructField(funded_amnt_inv,DoubleType,true),\n",
       " StructField(term,StringType,true),\n",
       " StructField(int_rate,StringType,true),\n",
       " StructField(installment,DoubleType,true),\n",
       " StructField(grade,StringType,true),\n",
       " StructField(sub_grade,StringType,true),\n",
       " StructField(emp_title,StringType,true),\n",
       " StructField(emp_length,StringType,true),\n",
       " StructField(home_ownership,StringType,true),\n",
       " StructField(annual_inc,DoubleType,true),\n",
       " StructField(verification_status,StringType,true),\n",
       " StructField(issue_d,StringType,true),\n",
       " StructField(loan_status,StringType,true),\n",
       " StructField(pymnt_plan,StringType,true),\n",
       " StructField(url,StringType,true),\n",
       " StructField(desc,StringType,true),\n",
       " StructField(purpose,StringType,true),\n",
       " StructField(title,StringType,true),\n",
       " StructField(zip_code,StringType,true),\n",
       " StructField(addr_state,StringType,true),\n",
       " StructField(dti,DoubleType,true),\n",
       " StructField(delinq_2yrs,IntegerType,true),\n",
       " StructField(earliest_cr_line,StringType,true),\n",
       " StructField(inq_last_6mths,IntegerType,true),\n",
       " StructField(mths_since_last_delinq,IntegerType,true),\n",
       " StructField(mths_since_last_record,IntegerType,true),\n",
       " StructField(open_acc,IntegerType,true),\n",
       " StructField(pub_rec,IntegerType,true),\n",
       " StructField(revol_bal,IntegerType,true),\n",
       " StructField(revol_util,StringType,true),\n",
       " StructField(total_acc,IntegerType,true),\n",
       " StructField(initial_list_status,StringType,true),\n",
       " StructField(out_prncp,DoubleType,true),\n",
       " StructField(out_prncp_inv,DoubleType,true),\n",
       " StructField(total_pymnt,DoubleType,true),\n",
       " StructField(total_pymnt_inv,DoubleType,true),\n",
       " StructField(total_rec_prncp,DoubleType,true),\n",
       " StructField(total_rec_int,DoubleType,true),\n",
       " StructField(total_rec_late_fee,DoubleType,true),\n",
       " StructField(recoveries,DoubleType,true),\n",
       " StructField(collection_recovery_fee,DoubleType,true),\n",
       " StructField(last_pymnt_d,StringType,true),\n",
       " StructField(last_pymnt_amnt,DoubleType,true),\n",
       " StructField(next_pymnt_d,StringType,true),\n",
       " StructField(last_credit_pull_d,StringType,true),\n",
       " StructField(collections_12_mths_ex_med,IntegerType,true),\n",
       " StructField(mths_since_last_major_derog,IntegerType,true),\n",
       " StructField(policy_code,IntegerType,true),\n",
       " StructField(application_type,StringType,true),\n",
       " StructField(annual_inc_joint,DoubleType,true),\n",
       " StructField(dti_joint,DoubleType,true),\n",
       " StructField(verification_status_joint,StringType,true),\n",
       " StructField(acc_now_delinq,IntegerType,true),\n",
       " StructField(tot_coll_amt,IntegerType,true),\n",
       " StructField(tot_cur_bal,IntegerType,true),\n",
       " StructField(open_acc_6m,IntegerType,true),\n",
       " StructField(open_act_il,IntegerType,true),\n",
       " StructField(open_il_12m,IntegerType,true),\n",
       " StructField(open_il_24m,IntegerType,true),\n",
       " StructField(mths_since_rcnt_il,IntegerType,true),\n",
       " StructField(total_bal_il,IntegerType,true),\n",
       " StructField(il_util,IntegerType,true),\n",
       " StructField(open_rv_12m,IntegerType,true),\n",
       " StructField(open_rv_24m,IntegerType,true),\n",
       " StructField(max_bal_bc,IntegerType,true),\n",
       " StructField(all_util,IntegerType,true),\n",
       " StructField(total_rev_hi_lim,IntegerType,true),\n",
       " StructField(inq_fi,IntegerType,true),\n",
       " StructField(total_cu_tl,IntegerType,true),\n",
       " StructField(inq_last_12m,IntegerType,true),\n",
       " StructField(acc_open_past_24mths,IntegerType,true),\n",
       " StructField(avg_cur_bal,IntegerType,true),\n",
       " StructField(bc_open_to_buy,IntegerType,true),\n",
       " StructField(bc_util,DoubleType,true),\n",
       " StructField(chargeoff_within_12_mths,IntegerType,true),\n",
       " StructField(delinq_amnt,IntegerType,true),\n",
       " StructField(mo_sin_old_il_acct,IntegerType,true),\n",
       " StructField(mo_sin_old_rev_tl_op,IntegerType,true),\n",
       " StructField(mo_sin_rcnt_rev_tl_op,IntegerType,true),\n",
       " StructField(mo_sin_rcnt_tl,IntegerType,true),\n",
       " StructField(mort_acc,IntegerType,true),\n",
       " StructField(mths_since_recent_bc,IntegerType,true),\n",
       " StructField(mths_since_recent_bc_dlq,IntegerType,true),\n",
       " StructField(mths_since_recent_inq,IntegerType,true),\n",
       " StructField(mths_since_recent_revol_delinq,IntegerType,true),\n",
       " StructField(num_accts_ever_120_pd,IntegerType,true),\n",
       " StructField(num_actv_bc_tl,IntegerType,true),\n",
       " StructField(num_actv_rev_tl,IntegerType,true),\n",
       " StructField(num_bc_sats,IntegerType,true),\n",
       " StructField(num_bc_tl,IntegerType,true),\n",
       " StructField(num_il_tl,IntegerType,true),\n",
       " StructField(num_op_rev_tl,IntegerType,true),\n",
       " StructField(num_rev_accts,IntegerType,true),\n",
       " StructField(num_rev_tl_bal_gt_0,IntegerType,true),\n",
       " StructField(num_sats,IntegerType,true),\n",
       " StructField(num_tl_120dpd_2m,IntegerType,true),\n",
       " StructField(num_tl_30dpd,IntegerType,true),\n",
       " StructField(num_tl_90g_dpd_24m,IntegerType,true),\n",
       " StructField(num_tl_op_past_12m,IntegerType,true),\n",
       " StructField(pct_tl_nvr_dlq,DoubleType,true),\n",
       " StructField(percent_bc_gt_75,DoubleType,true),\n",
       " StructField(pub_rec_bankruptcies,IntegerType,true),\n",
       " StructField(tax_liens,IntegerType,true),\n",
       " StructField(tot_hi_cred_lim,IntegerType,true),\n",
       " StructField(total_bal_ex_mort,IntegerType,true),\n",
       " StructField(total_bc_limit,IntegerType,true),\n",
       " StructField(total_il_high_credit_limit,IntegerType,true),\n",
       " StructField(revol_bal_joint,IntegerType,true),\n",
       " StructField(sec_app_earliest_cr_line,StringType,true),\n",
       " StructField(sec_app_inq_last_6mths,IntegerType,true),\n",
       " StructField(sec_app_mort_acc,IntegerType,true),\n",
       " StructField(sec_app_open_acc,IntegerType,true),\n",
       " StructField(sec_app_revol_util,DoubleType,true),\n",
       " StructField(sec_app_open_act_il,IntegerType,true),\n",
       " StructField(sec_app_num_rev_accts,IntegerType,true),\n",
       " StructField(sec_app_chargeoff_within_12_mths,IntegerType,true),\n",
       " StructField(sec_app_collections_12_mths_ex_med,IntegerType,true),\n",
       " StructField(sec_app_mths_since_last_major_derog,IntegerType,true),\n",
       " StructField(hardship_flag,StringType,true),\n",
       " StructField(hardship_type,StringType,true),\n",
       " StructField(hardship_reason,StringType,true),\n",
       " StructField(hardship_status,StringType,true),\n",
       " StructField(deferral_term,IntegerType,true),\n",
       " StructField(hardship_amount,DoubleType,true),\n",
       " StructField(hardship_start_date,StringType,true),\n",
       " StructField(hardship_end_date,StringType,true),\n",
       " StructField(payment_plan_start_date,StringType,true),\n",
       " StructField(hardship_length,IntegerType,true),\n",
       " StructField(hardship_dpd,IntegerType,true),\n",
       " StructField(hardship_loan_status,StringType,true),\n",
       " StructField(orig_projected_additional_accrued_interest,DoubleType,true),\n",
       " StructField(hardship_payoff_balance_amount,DoubleType,true),\n",
       " StructField(hardship_last_payment_amount,DoubleType,true),\n",
       " StructField(debt_settlement_flag,StringType,true),\n",
       " StructField(debt_settlement_flag_date,StringType,true),\n",
       " StructField(settlement_status,StringType,true),\n",
       " StructField(settlement_date,StringType,true),\n",
       " StructField(settlement_amount,IntegerType,true),\n",
       " StructField(settlement_percentage,DoubleType,true),\n",
       " StructField(settlement_term,IntegerType,true)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans_df.schema.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "through-brush",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "LoanStats_2018Q4.csv\n",
    "homeOwnership - The home ownership status provided by the borrower during registration. Our values are: RENT, OWN, MORTGAGE, OTHER.\n",
    "grade - LC assigned loan grade\n",
    "purpose - A category provided by the borrower for the loan request.\n",
    "intRate - Interest Rate on the loan\n",
    "addrState - The state provided by the borrower in the loan application\n",
    "loan_status - Current status of the loan\n",
    "application_type - Indicates whether the loan is an individual application or a joint application with two co-borrowers\n",
    "loan_amnt - The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.\n",
    "emp_length - Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.\n",
    "annual_inc - The self-reported annual income provided by the borrower during registration.\n",
    "dti - A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.\n",
    "dti_joint - A ratio calculated using the co-borrowers' total monthly payments on the total debt obligations, excluding mortgages and the requested LC loan, divided by the co-borrowers' combined self-reported monthly income\n",
    "delinq_2yrs - The number of 30+ days past-due incidences of delinquency in the borrower's credit file for the past 2 years\n",
    "revol_util - Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.\n",
    "total_acc - The total number of credit lines currently in the borrower's credit file\n",
    "num_tl_90g_dpd_24m - Number of accounts 90 or more days past due in last 24 months\n",
    "\n",
    "\n",
    "Analyse the Load Data\n",
    "a) Create a Hive Table using Pyspark\n",
    "\n",
    "\"\"\"\n",
    "''\n",
    "dir(loans_df.select('loan_amnt').take(1)[0])\n",
    "x = loans_df.select('*').take(1)[0].asDict()\n",
    "\n",
    "type(x.get('total_acc'))\n",
    "\n",
    "types_set= {type(v) for v in loans_df.select('*').take(1)[0].asDict().values()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "challenging-sapphire",
   "metadata": {},
   "outputs": [],
   "source": [
    "types_set\n",
    "\n",
    "type_to_hql_str_dict = {'float':'double',\n",
    "                        'int': 'bigint',\n",
    "                        'str': 'string'\n",
    "                       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "metropolitan-signal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "|        hw37|\n",
      "+------------+\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "'Unable to infer the schema. The schema specification is required to create the table `hw37`.`loans`.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o60.sql.\n: org.apache.spark.sql.AnalysisException: Unable to infer the schema. The schema specification is required to create the table `hw37`.`loans`.;\n\tat org.apache.spark.sql.hive.ResolveHiveSerdeTable$$anonfun$apply$1.applyOrElse(HiveStrategies.scala:104)\n\tat org.apache.spark.sql.hive.ResolveHiveSerdeTable$$anonfun$apply$1.applyOrElse(HiveStrategies.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsDown(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperators(AnalysisHelper.scala:73)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.hive.ResolveHiveSerdeTable.apply(HiveStrategies.scala:90)\n\tat org.apache.spark.sql.hive.ResolveHiveSerdeTable.apply(HiveStrategies.scala:44)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:643)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-44b3f9065951>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#         sleep(2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-44b3f9065951>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc_lst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m#         sleep(2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Unable to infer the schema. The schema specification is required to create the table `hw37`.`loans`.;'"
     ]
    }
   ],
   "source": [
    "dbname='hw37'\n",
    "loans_tn='loans'\n",
    "schema_fields = ['']\n",
    "\n",
    "spark.sql('show databases').show()\n",
    "c_lst = ['create database if not exists {}'.format(dbname),\n",
    "         'use {}'.format(dbname),\n",
    "         'show tables',\n",
    "         'create table if not exists {}'.format(loans_tn),\n",
    "        ]\n",
    "\n",
    "from time import sleep\n",
    "for cc in c_lst:\n",
    "    try:\n",
    "        spark.sql(cc).show()\n",
    "#         sleep(2)\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"b) Load the data into the cache\n",
    "c) Use Describe to show summary of Loan Data\n",
    "d) Show distinct emp_length \n",
    "e) Clean the emp_length column and create  new column e.g  emplength_cleaned using regular expression \n",
    "f) Clean “term” column and create another column named term_cleaned\n",
    "g) Display new schemas \n",
    "h) Create new temp Table for writing queries \n",
    "i) Show statistics of annual income vs loan amt\n",
    "j) Show statistics like correlations, std dev\n",
    "k) Apply when, otherwise, \n",
    "l) Where, coalesce,\n",
    "Analyse the dataset, visualization.\n",
    "\n",
    "2. Pyspark EDA & Data Analysis\n",
    "Dataset: Telco_Customer_Churn-89c80.csv\n",
    "Analyse the Load Data\n",
    "a) Create a Hive Table using Pyspark\n",
    "b) Load the data \n",
    "c) Use Describe to show summary of Loan Data\n",
    "d) Display new schemas \n",
    "e) Create new temp Table for writing queries \n",
    "f) Show statistics of annual income vs loan amt\n",
    "g) Show statistics like correlations, std dev\n",
    "h) apply when, otherwise, \n",
    "i) where etc. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-kruger",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
