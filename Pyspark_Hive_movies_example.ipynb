{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "traditional-employer",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    print(\"no sc to stop\")\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, HiveContext\n",
    "\n",
    "conf = SparkConf().setAppName('SparkHiveSession').setMaster('local[4]')\n",
    "sc = SparkContext.getOrCreate(conf = conf)\n",
    "\n",
    "\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "particular-token",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkHiveSession</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[4] appName=SparkHiveSession>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "external-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hive integration with spark session\n",
    "spark = (SparkSession.builder.appName('pysparkhiveintegration')\n",
    "        .config('spark.sql.warehouse.dir', '/user/hive/warehouse')\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "banned-teacher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "| bucketingdb|\n",
      "|   complexdb|\n",
      "|     default|\n",
      "|  employeedb|\n",
      "|  hivetestdb|\n",
      "|    hw271_db|\n",
      "|    hw272_db|\n",
      "|    hw273_db|\n",
      "|      jsondb|\n",
      "|   movies_db|\n",
      "| xmldatabase|\n",
      "|       xmldb|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "green-performer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('drop database if exists movies_db cascade').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "recreational-wings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('create database if not exists movies_db').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "maritime-password",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "| bucketingdb|\n",
      "|   complexdb|\n",
      "|     default|\n",
      "|  employeedb|\n",
      "|  hivetestdb|\n",
      "|    hw271_db|\n",
      "|    hw272_db|\n",
      "|    hw273_db|\n",
      "|      jsondb|\n",
      "|   movies_db|\n",
      "| xmldatabase|\n",
      "|       xmldb|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "limiting-chair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('use movies_db').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eight-tonight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "alert-afghanistan",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"create table if not exists movies \n",
    "    (movieId int, \n",
    "     title string,\n",
    "     genres string)\n",
    "     row format delimited\n",
    "     fields terminated by ','\n",
    "     stored as textfile     \n",
    "     tblproperties (\"skip.header.line.count\"=\"1\")\n",
    "    \"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"create table if not exists ratings\n",
    "            (userId int, movieId int, rating float, timestamp string)\n",
    "            stored as ORC\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "smart-result",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "| database|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|movies_db|   movies|      false|\n",
      "|movies_db|  ratings|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql(\"\"\" \"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"show tables\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "normal-mailman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|              userId|                 int|   null|\n",
      "|             movieId|                 int|   null|\n",
      "|              rating|               float|   null|\n",
      "|           timestamp|              string|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|            Database|           movies_db|       |\n",
      "|               Table|             ratings|       |\n",
      "|               Owner|              hadoop|       |\n",
      "|        Created Time|Thu Jan 21 21:14:...|       |\n",
      "|         Last Access|Thu Jan 01 05:30:...|       |\n",
      "|          Created By|         Spark 2.4.7|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|                hive|       |\n",
      "|    Table Properties|[transient_lastDd...|       |\n",
      "|            Location|hdfs://localhost:...|       |\n",
      "|       Serde Library|org.apache.hadoop...|       |\n",
      "|         InputFormat|org.apache.hadoop...|       |\n",
      "|        OutputFormat|org.apache.hadoop...|       |\n",
      "|  Storage Properties|[serialization.fo...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"describe formatted ratings\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "loving-admission",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"load data local inpath '/home/hadoop/data/ml-latest/movies.csv'\n",
    "overwrite into table movies\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "middle-scotland",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from movies limit 5\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "republican-cuisine",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Inserting data into rating table using spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "entire-meaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import * # OH NO!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "genuine-nowhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('UserId', IntegerType()),\n",
    "    StructField('movieId', IntegerType()),\n",
    "    StructField('rating', DoubleType()),\n",
    "    StructField('timestamp', IntegerType()) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "limiting-generic",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = spark.read.csv(\"file:///home/hadoop/data/ml-latest/ratings.csv\", schema=schema, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "worse-category",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|UserId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|    307|   3.5|1256677221|\n",
      "|     1|    481|   3.5|1256677456|\n",
      "|     1|   1091|   1.5|1256677471|\n",
      "|     1|   1257|   4.5|1256677460|\n",
      "|     1|   1449|   4.5|1256677264|\n",
      "+------+-------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rating_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "postal-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df.createOrReplaceTempView(\"rating_df_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "civic-knitting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql(\"\"\" \"\"\").show()\n",
    "spark.sql(\"\"\"insert into table ratings select * from rating_df_table\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "external-nevada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating|timestamp |\n",
      "+------+-------+------+----------+\n",
      "|51979 |5941   |2.5   |1439599249|\n",
      "|51979 |5942   |3.5   |1439677197|\n",
      "|51979 |5943   |2.5   |1439574637|\n",
      "|51979 |5952   |4.0   |1439573853|\n",
      "|51979 |5958   |3.0   |1439647221|\n",
      "|51979 |5969   |1.5   |1439692109|\n",
      "|51979 |5970   |3.0   |1439666647|\n",
      "|51979 |5989   |4.5   |1528789425|\n",
      "|51979 |5991   |3.5   |1439768072|\n",
      "|51979 |5995   |3.5   |1439573967|\n",
      "+------+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from ratings limit 10\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "czech-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pyspark RDD broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "reasonable-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = {\"NY\": \"New York\", \n",
    "          \"CA\": \"California\", \n",
    "          \"FL\": \"Flo Rida\",\n",
    "          \"TX\": \"Texas\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "meaning-avatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcastStates = sc.broadcast(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "played-cement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.broadcast.Broadcast at 0x7f355fe1d7b8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastStates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "temporal-leave",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method broadcast in module pyspark.context:\n",
      "\n",
      "broadcast(value) method of pyspark.context.SparkContext instance\n",
      "    Broadcast a read-only variable to the cluster, returning a\n",
      "    L{Broadcast<pyspark.broadcast.Broadcast>}\n",
      "    object for reading it in distributed functions. The variable will\n",
      "    be sent to each cluster only once.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sc.broadcast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "supported-porcelain",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= [\n",
    "    ('Rock', 'Shi', 'USA', 'CA'),\n",
    "    ('Emy', 'Kay', 'USA', 'NY'),\n",
    "    ('Michael', 'Rose', 'USA', 'NY'),\n",
    "    ('Johnny','Cash','USA','TX'),\n",
    "    ('Joe','Exotic','USA','FL'),\n",
    "      ]\n",
    "\n",
    "rdd_4 = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "scientific-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "stupid-horizon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rock', 'Shi', 'USA', 'California'),\n",
       " ('Emy', 'Kay', 'USA', 'New York'),\n",
       " ('Michael', 'Rose', 'USA', 'New York'),\n",
       " ('Johnny', 'Cash', 'USA', 'Texas'),\n",
       " ('Joe', 'Exotic', 'USA', 'Flo Rida')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_4.map(lambda x: (x[0],x[1],x[2], state_convert(x[3])) ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "conventional-modem",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Broadcast variables allow to keep read only variable cached on each machine rather than shipping copy of it with tasks.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Broadcast variables allow to keep read only variable \\\n",
    "cached on each machine rather than shipping copy of it with tasks.\\\n",
    "To give every worker node a copy of large input dataset \\\n",
    "in an efficient manner.\\\n",
    "\\\n",
    "You don't broadcast RDD but values are to all executor nodes that is used multiple times while processing RDD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-asset",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "furnished-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pyspark repartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "infectious-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "plastic-loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from local[4]: 6\n"
     ]
    }
   ],
   "source": [
    "rdd5 = sc.parallelize( (0,20), 6 )\n",
    "\n",
    "print(\"from local[4]: \" + str(rdd5.getNumPartitions()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "curious-recommendation",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd5.saveAsTextFile(\"file:///home/hadoop/parts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "centered-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd6 = rdd5.coalesce(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cloudy-trash",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(rdd6.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-attachment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
