{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "surprised-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    print(\"no sc to stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "corporate-printing",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eligible-curtis",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('PysparkNLP')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "desperate-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pressed-canal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            sentence|\n",
      "+---+--------------------+\n",
      "|  0|this is Spark ses...|\n",
      "|  1|I wish we could h...|\n",
      "|  2|for text analysis...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sen_df = spark.createDataFrame([\n",
    "    (0, \"this is Spark session on NLP\"),\n",
    "    (1, \"I wish we could have learned NLP with python.\"),\n",
    "    (2, \"for text analysis ml algorithms are logistic, naive bayes model\"),\n",
    "    ], schema= ['id', 'sentence'])\n",
    "\n",
    "sen_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "medical-newman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer & Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acknowledged-underwear",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            sentence|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|this is Spark ses...|[this, is, spark,...|\n",
      "|  1|I wish we could h...|[i, wish, we, cou...|\n",
      "|  2|for text analysis...|[for, text, analy...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol='sentence', outputCol='words')\n",
    "regex_tokenizer = RegexTokenizer(inputCol='sentence', outputCol='words', pattern='\\\\W')\n",
    "\n",
    "#count_tokens = udf(lamba s,w: s.count(w))\n",
    "count_tokens = udf(lambda w: len(w) , IntegerType())\n",
    "\n",
    "tokenized=tokenizer.transform(sen_df)\n",
    "\n",
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "driving-regression",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------+\n",
      "| id|            sentence|               words|tokens|\n",
      "+---+--------------------+--------------------+------+\n",
      "|  0|this is Spark ses...|[this, is, spark,...|     6|\n",
      "|  1|I wish we could h...|[i, wish, we, cou...|     9|\n",
      "|  2|for text analysis...|[for, text, analy...|    10|\n",
      "+---+--------------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized.withColumn('tokens', count_tokens('words')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "induced-grounds",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            sentence|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|this is Spark ses...|[this, is, spark,...|\n",
      "|  1|I wish we could h...|[i, wish, we, cou...|\n",
      "|  2|for text analysis...|[for, text, analy...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regex_tokenizer = RegexTokenizer(inputCol='sentence', outputCol='words', pattern=r'\\W')\n",
    "regex_tokenized = regex_tokenizer.transform(sen_df)\n",
    "regex_tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "sunset-yacht",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+---------------------------------------------------------------+\n",
      "|words                                                                    |filtered                                                       |\n",
      "+-------------------------------------------------------------------------+---------------------------------------------------------------+\n",
      "|[this, is, spark, session, on, nlp]                                      |[spark, session, nlp]                                          |\n",
      "|[i, wish, we, could, have, learned, nlp, with, python]                   |[wish, learned, nlp, python]                                   |\n",
      "|[for, text, analysis, ml, algorithms, are, logistic, naive, bayes, model]|[text, analysis, ml, algorithms, logistic, naive, bayes, model]|\n",
      "+-------------------------------------------------------------------------+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "filtered_df = StopWordsRemover(inputCol='words', outputCol='filtered')\n",
    "filtered_df.transform(regex_tokenized).select('words', 'filtered').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "portable-concentrate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nReview of Restaurants\\n\\nsentiment analysis using twitter\\n\\nnegative/positive/neutral sentiment expressed on topic\\n\\n(The ambience of the cafe was good but food was pathetic) -> neutral\\n(The ambience of the cafe was good) ->  positive\\n(but food was pathetic) -> negative\\n\\n\\n(the food was not good) - > neutral?\\n(the food) (was not) (good) \\n(the food) (was) (not good) -> negative\\n\\n\\n(food not good)\\n(food not) (not good)\\n\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Review of Restaurants\n",
    "\n",
    "sentiment analysis using twitter\n",
    "\n",
    "negative/positive/neutral sentiment expressed on topic\n",
    "\n",
    "(The ambience of the cafe was good but food was pathetic) -> neutral\n",
    "(The ambience of the cafe was good) ->  positive\n",
    "(but food was pathetic) -> negative\n",
    "\n",
    "\n",
    "(the food was not good) - > neutral?\n",
    "(the food) (was not) (good) \n",
    "(the food) (was) (not good) -> negative\n",
    "\n",
    "\n",
    "(food not good)\n",
    "(food not) (not good)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "historical-alarm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|                text|\n",
      "+---+--------------------+\n",
      "|  0|[Hi, I, know, abo...|\n",
      "|  1|[I, wish, we, cou...|\n",
      "|  2|[For, text, analy...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "wordDataFrame= spark.createDataFrame([\n",
    "    (0, \"Hi I know about nlp\".split()),\n",
    "    (1, \"I wish we could have learned nlp\".split()),\n",
    "    (2, \"For text analysis ml algorithms are logistic, naive bayes model\".split()),        \n",
    "], ['id', 'text'])\n",
    "\n",
    "wordDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "boxed-spanking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------+\n",
      "|id |text                                                                      |grams                                                                                                                          |\n",
      "+---+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0  |[Hi, I, know, about, nlp]                                                 |[Hi I, I know, know about, about nlp]                                                                                          |\n",
      "|1  |[I, wish, we, could, have, learned, nlp]                                  |[I wish, wish we, we could, could have, have learned, learned nlp]                                                             |\n",
      "|2  |[For, text, analysis, ml, algorithms, are, logistic,, naive, bayes, model]|[For text, text analysis, analysis ml, ml algorithms, algorithms are, are logistic,, logistic, naive, naive bayes, bayes model]|\n",
      "+---+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngram = NGram(inputCol='text', outputCol='grams')\n",
    "ngram.transform(wordDataFrame).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "instrumental-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Term frequency (TF) and IDF (inverse document frequency)/home/hadoop/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "whole-victorian",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "sixth-beads",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------------------------------+\n",
      "|label|sentence                                                       |\n",
      "+-----+---------------------------------------------------------------+\n",
      "|0.0  |this is Spark session on NLP                                   |\n",
      "|0.0  |I wish we could have learned NLP with python.                  |\n",
      "|1.0  |for text analysis ml algorithms are logistic, naive bayes model|\n",
      "+-----+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceDF = spark.createDataFrame([\n",
    "    (0.0, \"this is Spark session on NLP\"),\n",
    "    (0.0, \"I wish we could have learned NLP with python.\"),\n",
    "    (1.0, \"for text analysis ml algorithms are logistic, naive bayes model\"),\n",
    "    ], schema= ['label', 'sentence'])\n",
    "\n",
    "sentenceDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fatty-optimization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------------------------------+--------------------------------------------------------------------------+\n",
      "|label|sentence                                                       |words                                                                     |\n",
      "+-----+---------------------------------------------------------------+--------------------------------------------------------------------------+\n",
      "|0.0  |this is Spark session on NLP                                   |[this, is, spark, session, on, nlp]                                       |\n",
      "|0.0  |I wish we could have learned NLP with python.                  |[i, wish, we, could, have, learned, nlp, with, python.]                   |\n",
      "|1.0  |for text analysis ml algorithms are logistic, naive bayes model|[for, text, analysis, ml, algorithms, are, logistic,, naive, bayes, model]|\n",
      "+-----+---------------------------------------------------------------+--------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer=Tokenizer(inputCol='sentence', outputCol='words')\n",
    "words_df=tokenizer.transform(sentenceDF)\n",
    "words_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "similar-greene",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------------------------------+--------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+\n",
      "|label|sentence                                                       |words                                                                     |rawFeatures                                                                                                           |\n",
      "+-----+---------------------------------------------------------------+--------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+\n",
      "|0.0  |this is Spark session on NLP                                   |[this, is, spark, session, on, nlp]                                       |(262144,[9677,15889,100258,108541,116034,234657],[1.0,1.0,1.0,1.0,1.0,1.0])                                           |\n",
      "|0.0  |I wish we could have learned NLP with python.                  |[i, wish, we, could, have, learned, nlp, with, python.]                   |(262144,[20719,24417,116034,126466,139934,142830,147489,147765,253475],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])         |\n",
      "|1.0  |for text analysis ml algorithms are logistic, naive bayes model|[for, text, analysis, ml, algorithms, are, logistic,, naive, bayes, model]|(262144,[16332,25817,36578,92225,143985,150224,158432,167122,183588,254285],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "+-----+---------------------------------------------------------------+--------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashing_tf = sentenceDF = spark.createDataFrame([\n",
    "    (0, 'python spark hive, spark, hadoop'),\n",
    "    (0, 'a b b b c'),\n",
    "    (1, 'c hadoop hive spark hadoop hive')\n",
    "], ['label', 'sentence'])\n",
    "\n",
    "sentenceDF.show()\n",
    "(inputCol='words', outputCol='rawFeatures')\n",
    "\n",
    "featurized_df = hashing_tf.transform(words_df)\n",
    "featurized_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "communist-expense",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------+\n",
      "|rawFeatures                                                                                                           |\n",
      "+----------------------------------------------------------------------------------------------------------------------+\n",
      "|(262144,[9677,15889,100258,108541,116034,234657],[1.0,1.0,1.0,1.0,1.0,1.0])                                           |\n",
      "|(262144,[20719,24417,116034,126466,139934,142830,147489,147765,253475],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])         |\n",
      "|(262144,[16332,25817,36578,92225,143985,150224,158432,167122,183588,254285],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "+----------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurized_df.select('rawFeatures').show(truncate=False)\n",
    "\n",
    "# (document_hash, [word_hashes], [frequencies] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "outer-council",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class HashingTF in module pyspark.ml.feature:\n",
      "\n",
      "class HashingTF(pyspark.ml.wrapper.JavaTransformer, pyspark.ml.param.shared.HasInputCol, pyspark.ml.param.shared.HasOutputCol, pyspark.ml.param.shared.HasNumFeatures, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.JavaMLWritable)\n",
      " |  Maps a sequence of terms to their term frequencies using the hashing trick.\n",
      " |  Currently we use Austin Appleby's MurmurHash 3 algorithm (MurmurHash3_x86_32)\n",
      " |  to calculate the hash code value for the term object.\n",
      " |  Since a simple modulo is used to transform the hash function to a column index,\n",
      " |  it is advisable to use a power of two as the numFeatures parameter;\n",
      " |  otherwise the features will not be mapped evenly to the columns.\n",
      " |  \n",
      " |  >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], [\"words\"])\n",
      " |  >>> hashingTF = HashingTF(numFeatures=10, inputCol=\"words\", outputCol=\"features\")\n",
      " |  >>> hashingTF.transform(df).head().features\n",
      " |  SparseVector(10, {0: 1.0, 1: 1.0, 2: 1.0})\n",
      " |  >>> hashingTF.setParams(outputCol=\"freqs\").transform(df).head().freqs\n",
      " |  SparseVector(10, {0: 1.0, 1: 1.0, 2: 1.0})\n",
      " |  >>> params = {hashingTF.numFeatures: 5, hashingTF.outputCol: \"vector\"}\n",
      " |  >>> hashingTF.transform(df, params).head().vector\n",
      " |  SparseVector(5, {0: 1.0, 1: 1.0, 2: 1.0})\n",
      " |  >>> hashingTFPath = temp_path + \"/hashing-tf\"\n",
      " |  >>> hashingTF.save(hashingTFPath)\n",
      " |  >>> loadedHashingTF = HashingTF.load(hashingTFPath)\n",
      " |  >>> loadedHashingTF.getNumFeatures() == hashingTF.getNumFeatures()\n",
      " |  True\n",
      " |  \n",
      " |  .. versionadded:: 1.3.0\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      HashingTF\n",
      " |      pyspark.ml.wrapper.JavaTransformer\n",
      " |      pyspark.ml.wrapper.JavaParams\n",
      " |      pyspark.ml.wrapper.JavaWrapper\n",
      " |      pyspark.ml.base.Transformer\n",
      " |      pyspark.ml.param.shared.HasInputCol\n",
      " |      pyspark.ml.param.shared.HasOutputCol\n",
      " |      pyspark.ml.param.shared.HasNumFeatures\n",
      " |      pyspark.ml.param.Params\n",
      " |      pyspark.ml.util.Identifiable\n",
      " |      pyspark.ml.util.JavaMLReadable\n",
      " |      pyspark.ml.util.MLReadable\n",
      " |      pyspark.ml.util.JavaMLWritable\n",
      " |      pyspark.ml.util.MLWritable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, numFeatures=262144, binary=False, inputCol=None, outputCol=None)\n",
      " |      __init__(self, numFeatures=1 << 18, binary=False, inputCol=None, outputCol=None)\n",
      " |  \n",
      " |  getBinary(self)\n",
      " |      Gets the value of binary or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  setBinary(self, value)\n",
      " |      Sets the value of :py:attr:`binary`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  setParams(self, numFeatures=262144, binary=False, inputCol=None, outputCol=None)\n",
      " |      setParams(self, numFeatures=1 << 18, binary=False, inputCol=None, outputCol=None)\n",
      " |      Sets params for this HashingTF.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  binary = Param(parent='undefined', name='binary', doc='If...ents rathe...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.wrapper.JavaTransformer:\n",
      " |  \n",
      " |  __metaclass__ = <class 'abc.ABCMeta'>\n",
      " |      Metaclass for defining Abstract Base Classes (ABCs).\n",
      " |      \n",
      " |      Use this metaclass to create an ABC.  An ABC can be subclassed\n",
      " |      directly, and then acts as a mix-in class.  You can also register\n",
      " |      unrelated concrete classes (even built-in classes) and unrelated\n",
      " |      ABCs as 'virtual subclasses' -- these and their descendants will\n",
      " |      be considered subclasses of the registering ABC by the built-in\n",
      " |      issubclass() function, but the registering ABC won't show up in\n",
      " |      their MRO (Method Resolution Order) nor will method\n",
      " |      implementations defined by the registering ABC be callable (not\n",
      " |      even via super()).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n",
      " |  \n",
      " |  copy(self, extra=None)\n",
      " |      Creates a copy of this instance with the same uid and some\n",
      " |      extra params. This implementation first calls Params.copy and\n",
      " |      then make a copy of the companion Java pipeline component with\n",
      " |      extra params. So both the Python wrapper and the Java pipeline\n",
      " |      component get copied.\n",
      " |      \n",
      " |      :param extra: Extra parameters to copy to the new instance\n",
      " |      :return: Copy of this instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.base.Transformer:\n",
      " |  \n",
      " |  transform(self, dataset, params=None)\n",
      " |      Transforms the input dataset with optional parameters.\n",
      " |      \n",
      " |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`\n",
      " |      :param params: an optional param map that overrides embedded params.\n",
      " |      :returns: transformed dataset\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasInputCol:\n",
      " |  \n",
      " |  getInputCol(self)\n",
      " |      Gets the value of inputCol or its default value.\n",
      " |  \n",
      " |  setInputCol(self, value)\n",
      " |      Sets the value of :py:attr:`inputCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasInputCol:\n",
      " |  \n",
      " |  inputCol = Param(parent='undefined', name='inputCol', doc='input colum...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasOutputCol:\n",
      " |  \n",
      " |  getOutputCol(self)\n",
      " |      Gets the value of outputCol or its default value.\n",
      " |  \n",
      " |  setOutputCol(self, value)\n",
      " |      Sets the value of :py:attr:`outputCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasOutputCol:\n",
      " |  \n",
      " |  outputCol = Param(parent='undefined', name='outputCol', doc='output co...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasNumFeatures:\n",
      " |  \n",
      " |  getNumFeatures(self)\n",
      " |      Gets the value of numFeatures or its default value.\n",
      " |  \n",
      " |  setNumFeatures(self, value)\n",
      " |      Sets the value of :py:attr:`numFeatures`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasNumFeatures:\n",
      " |  \n",
      " |  numFeatures = Param(parent='undefined', name='numFeatures', doc='numbe...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  explainParam(self, param)\n",
      " |      Explains a single param and returns its name, doc, and optional\n",
      " |      default value and user-supplied value in a string.\n",
      " |  \n",
      " |  explainParams(self)\n",
      " |      Returns the documentation of all params with their optionally\n",
      " |      default values and user-supplied values.\n",
      " |  \n",
      " |  extractParamMap(self, extra=None)\n",
      " |      Extracts the embedded default param values and user-supplied\n",
      " |      values, and then merges them with extra values from input into\n",
      " |      a flat param map, where the latter value is used if there exist\n",
      " |      conflicts, i.e., with ordering: default param values <\n",
      " |      user-supplied values < extra.\n",
      " |      \n",
      " |      :param extra: extra param values\n",
      " |      :return: merged param map\n",
      " |  \n",
      " |  getOrDefault(self, param)\n",
      " |      Gets the value of a param in the user-supplied param map or its\n",
      " |      default value. Raises an error if neither is set.\n",
      " |  \n",
      " |  getParam(self, paramName)\n",
      " |      Gets a param by its name.\n",
      " |  \n",
      " |  hasDefault(self, param)\n",
      " |      Checks whether a param has a default value.\n",
      " |  \n",
      " |  hasParam(self, paramName)\n",
      " |      Tests whether this instance contains a param with a given\n",
      " |      (string) name.\n",
      " |  \n",
      " |  isDefined(self, param)\n",
      " |      Checks whether a param is explicitly set by user or has\n",
      " |      a default value.\n",
      " |  \n",
      " |  isSet(self, param)\n",
      " |      Checks whether a param is explicitly set by user.\n",
      " |  \n",
      " |  set(self, param, value)\n",
      " |      Sets a parameter in the embedded param map.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  params\n",
      " |      Returns all params ordered by name. The default implementation\n",
      " |      uses :py:func:`dir` to get all attributes of type\n",
      " |      :py:class:`Param`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.Identifiable:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n",
      " |  \n",
      " |  read() from builtins.type\n",
      " |      Returns an MLReader instance for this class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.MLReadable:\n",
      " |  \n",
      " |  load(path) from builtins.type\n",
      " |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n",
      " |  \n",
      " |  write(self)\n",
      " |      Returns an MLWriter instance for this ML instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.MLWritable:\n",
      " |  \n",
      " |  save(self, path)\n",
      " |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(HashingTF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "immediate-madison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------------------------------+--------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|sentence                                                       |words                                                                     |rawFeatures                                                                                                           |features                                                                                                                                                                                                                                                                    |\n",
      "+-----+---------------------------------------------------------------+--------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0.0  |this is Spark session on NLP                                   |[this, is, spark, session, on, nlp]                                       |(262144,[9677,15889,100258,108541,116034,234657],[1.0,1.0,1.0,1.0,1.0,1.0])                                           |(262144,[9677,15889,100258,108541,116034,234657],[0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.28768207245178085,0.6931471805599453])                                                                                                      |\n",
      "|0.0  |I wish we could have learned NLP with python.                  |[i, wish, we, could, have, learned, nlp, with, python.]                   |(262144,[20719,24417,116034,126466,139934,142830,147489,147765,253475],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])         |(262144,[20719,24417,116034,126466,139934,142830,147489,147765,253475],[0.6931471805599453,0.6931471805599453,0.28768207245178085,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])                       |\n",
      "|1.0  |for text analysis ml algorithms are logistic, naive bayes model|[for, text, analysis, ml, algorithms, are, logistic,, naive, bayes, model]|(262144,[16332,25817,36578,92225,143985,150224,158432,167122,183588,254285],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|(262144,[16332,25817,36578,92225,143985,150224,158432,167122,183588,254285],[0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])|\n",
      "+-----+---------------------------------------------------------------+--------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idf= IDF(inputCol='rawFeatures', outputCol='features')\n",
    "idf_model = idf.fit(featurized_df)\n",
    "rescaled_df = idf_model.transform(featurized_df)\n",
    "\n",
    "rescaled_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "supposed-valve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            sentence|               words|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|python spark hive...|[python, spark, h...|\n",
      "|    0|           a b b b c|     [a, b, b, b, c]|\n",
      "|    1|c hadoop hive spa...|[c, hadoop, hive,...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "sentenceDF = spark.createDataFrame([\n",
    "    (0, 'python spark hive spark hadoop'),\n",
    "    (0, 'a b b b c'),\n",
    "    (1, 'c hadoop hive spark hadoop hive')\n",
    "], ['label', 'sentence'])\n",
    "\n",
    "#sentenceDF.show()\n",
    "\n",
    "wordsDF=tokenizer.transform(sentenceDF)\n",
    "wordsDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "roman-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv =  CountVectorizer(inputCol='words', outputCol='features', vocabSize=262144, minDF=2.0)\n",
    "cv =  CountVectorizer(inputCol='words', outputCol='features', vocabSize=6,  minDF=2.0)\n",
    "# minDF : minimum document frequency\n",
    "# vocabSize : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "religious-organizer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------+--------------------------------------+-------------------------------+\n",
      "|label|sentence                       |words                                 |features                       |\n",
      "+-----+-------------------------------+--------------------------------------+-------------------------------+\n",
      "|0    |python spark hive spark hadoop |[python, spark, hive, spark, hadoop]  |(4,[0,1,2],[1.0,2.0,1.0])      |\n",
      "|0    |a b b b c                      |[a, b, b, b, c]                       |(4,[3],[1.0])                  |\n",
      "|1    |c hadoop hive spark hadoop hive|[c, hadoop, hive, spark, hadoop, hive]|(4,[0,1,2,3],[2.0,1.0,2.0,1.0])|\n",
      "+-----+-------------------------------+--------------------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_model = cv.fit(wordsDF)\n",
    "cv_df = cv_model.transform(wordsDF)\n",
    "cv_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "marked-soviet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CountVectorizer in module pyspark.ml.feature:\n",
      "\n",
      "class CountVectorizer(pyspark.ml.wrapper.JavaEstimator, _CountVectorizerParams, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.JavaMLWritable)\n",
      " |  Extracts a vocabulary from document collections and generates a :py:attr:`CountVectorizerModel`.\n",
      " |  \n",
      " |  >>> df = spark.createDataFrame(\n",
      " |  ...    [(0, [\"a\", \"b\", \"c\"]), (1, [\"a\", \"b\", \"b\", \"c\", \"a\"])],\n",
      " |  ...    [\"label\", \"raw\"])\n",
      " |  >>> cv = CountVectorizer(inputCol=\"raw\", outputCol=\"vectors\")\n",
      " |  >>> model = cv.fit(df)\n",
      " |  >>> model.transform(df).show(truncate=False)\n",
      " |  +-----+---------------+-------------------------+\n",
      " |  |label|raw            |vectors                  |\n",
      " |  +-----+---------------+-------------------------+\n",
      " |  |0    |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      " |  |1    |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      " |  +-----+---------------+-------------------------+\n",
      " |  ...\n",
      " |  >>> sorted(model.vocabulary) == ['a', 'b', 'c']\n",
      " |  True\n",
      " |  >>> countVectorizerPath = temp_path + \"/count-vectorizer\"\n",
      " |  >>> cv.save(countVectorizerPath)\n",
      " |  >>> loadedCv = CountVectorizer.load(countVectorizerPath)\n",
      " |  >>> loadedCv.getMinDF() == cv.getMinDF()\n",
      " |  True\n",
      " |  >>> loadedCv.getMinTF() == cv.getMinTF()\n",
      " |  True\n",
      " |  >>> loadedCv.getVocabSize() == cv.getVocabSize()\n",
      " |  True\n",
      " |  >>> modelPath = temp_path + \"/count-vectorizer-model\"\n",
      " |  >>> model.save(modelPath)\n",
      " |  >>> loadedModel = CountVectorizerModel.load(modelPath)\n",
      " |  >>> loadedModel.vocabulary == model.vocabulary\n",
      " |  True\n",
      " |  >>> fromVocabModel = CountVectorizerModel.from_vocabulary([\"a\", \"b\", \"c\"],\n",
      " |  ...     inputCol=\"raw\", outputCol=\"vectors\")\n",
      " |  >>> fromVocabModel.transform(df).show(truncate=False)\n",
      " |  +-----+---------------+-------------------------+\n",
      " |  |label|raw            |vectors                  |\n",
      " |  +-----+---------------+-------------------------+\n",
      " |  |0    |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      " |  |1    |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      " |  +-----+---------------+-------------------------+\n",
      " |  ...\n",
      " |  \n",
      " |  .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CountVectorizer\n",
      " |      pyspark.ml.wrapper.JavaEstimator\n",
      " |      _CountVectorizerParams\n",
      " |      pyspark.ml.wrapper.JavaParams\n",
      " |      pyspark.ml.wrapper.JavaWrapper\n",
      " |      pyspark.ml.base.Estimator\n",
      " |      pyspark.ml.param.shared.HasInputCol\n",
      " |      pyspark.ml.param.shared.HasOutputCol\n",
      " |      pyspark.ml.param.Params\n",
      " |      pyspark.ml.util.Identifiable\n",
      " |      pyspark.ml.util.JavaMLReadable\n",
      " |      pyspark.ml.util.MLReadable\n",
      " |      pyspark.ml.util.JavaMLWritable\n",
      " |      pyspark.ml.util.MLWritable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, minTF=1.0, minDF=1.0, maxDF=9223372036854775807, vocabSize=262144, binary=False, inputCol=None, outputCol=None)\n",
      " |      __init__(self, minTF=1.0, minDF=1.0, maxDF=2 ** 63 - 1, vocabSize=1 << 18, binary=False,                 inputCol=None,outputCol=None)\n",
      " |  \n",
      " |  setBinary(self, value)\n",
      " |      Sets the value of :py:attr:`binary`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  setMaxDF(self, value)\n",
      " |      Sets the value of :py:attr:`maxDF`.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |  \n",
      " |  setMinDF(self, value)\n",
      " |      Sets the value of :py:attr:`minDF`.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  setMinTF(self, value)\n",
      " |      Sets the value of :py:attr:`minTF`.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  setParams(self, minTF=1.0, minDF=1.0, maxDF=9223372036854775807, vocabSize=262144, binary=False, inputCol=None, outputCol=None)\n",
      " |      setParams(self, minTF=1.0, minDF=1.0, maxDF=2 ** 63 - 1, vocabSize=1 << 18, binary=False,                  inputCol=None, outputCol=None)\n",
      " |      Set the params for the CountVectorizer\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  setVocabSize(self, value)\n",
      " |      Sets the value of :py:attr:`vocabSize`.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.wrapper.JavaEstimator:\n",
      " |  \n",
      " |  __metaclass__ = <class 'abc.ABCMeta'>\n",
      " |      Metaclass for defining Abstract Base Classes (ABCs).\n",
      " |      \n",
      " |      Use this metaclass to create an ABC.  An ABC can be subclassed\n",
      " |      directly, and then acts as a mix-in class.  You can also register\n",
      " |      unrelated concrete classes (even built-in classes) and unrelated\n",
      " |      ABCs as 'virtual subclasses' -- these and their descendants will\n",
      " |      be considered subclasses of the registering ABC by the built-in\n",
      " |      issubclass() function, but the registering ABC won't show up in\n",
      " |      their MRO (Method Resolution Order) nor will method\n",
      " |      implementations defined by the registering ABC be callable (not\n",
      " |      even via super()).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _CountVectorizerParams:\n",
      " |  \n",
      " |  getBinary(self)\n",
      " |      Gets the value of binary or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  getMaxDF(self)\n",
      " |      Gets the value of maxDF or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |  \n",
      " |  getMinDF(self)\n",
      " |      Gets the value of minDF or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  getMinTF(self)\n",
      " |      Gets the value of minTF or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  getVocabSize(self)\n",
      " |      Gets the value of vocabSize or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from _CountVectorizerParams:\n",
      " |  \n",
      " |  binary = Param(parent='undefined', name='binary', doc='Bi...vents rath...\n",
      " |  \n",
      " |  maxDF = Param(parent='undefined', name='maxDF', doc='Spe...ts the term...\n",
      " |  \n",
      " |  minDF = Param(parent='undefined', name='minDF', doc='Spe...pecifies th...\n",
      " |  \n",
      " |  minTF = Param(parent='undefined', name='minTF', doc=\"Fil...rModel and ...\n",
      " |  \n",
      " |  vocabSize = Param(parent='undefined', name='vocabSize', doc='max size ...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n",
      " |  \n",
      " |  copy(self, extra=None)\n",
      " |      Creates a copy of this instance with the same uid and some\n",
      " |      extra params. This implementation first calls Params.copy and\n",
      " |      then make a copy of the companion Java pipeline component with\n",
      " |      extra params. So both the Python wrapper and the Java pipeline\n",
      " |      component get copied.\n",
      " |      \n",
      " |      :param extra: Extra parameters to copy to the new instance\n",
      " |      :return: Copy of this instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.base.Estimator:\n",
      " |  \n",
      " |  fit(self, dataset, params=None)\n",
      " |      Fits a model to the input dataset with optional parameters.\n",
      " |      \n",
      " |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`\n",
      " |      :param params: an optional param map that overrides embedded params. If a list/tuple of\n",
      " |                     param maps is given, this calls fit on each param map and returns a list of\n",
      " |                     models.\n",
      " |      :returns: fitted model(s)\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |  \n",
      " |  fitMultiple(self, dataset, paramMaps)\n",
      " |      Fits a model to the input dataset for each param map in `paramMaps`.\n",
      " |      \n",
      " |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`.\n",
      " |      :param paramMaps: A Sequence of param maps.\n",
      " |      :return: A thread safe iterable which contains one model for each param map. Each\n",
      " |               call to `next(modelIterator)` will return `(index, model)` where model was fit\n",
      " |               using `paramMaps[index]`. `index` values may not be sequential.\n",
      " |      \n",
      " |      .. note:: DeveloperApi\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasInputCol:\n",
      " |  \n",
      " |  getInputCol(self)\n",
      " |      Gets the value of inputCol or its default value.\n",
      " |  \n",
      " |  setInputCol(self, value)\n",
      " |      Sets the value of :py:attr:`inputCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasInputCol:\n",
      " |  \n",
      " |  inputCol = Param(parent='undefined', name='inputCol', doc='input colum...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasOutputCol:\n",
      " |  \n",
      " |  getOutputCol(self)\n",
      " |      Gets the value of outputCol or its default value.\n",
      " |  \n",
      " |  setOutputCol(self, value)\n",
      " |      Sets the value of :py:attr:`outputCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasOutputCol:\n",
      " |  \n",
      " |  outputCol = Param(parent='undefined', name='outputCol', doc='output co...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  explainParam(self, param)\n",
      " |      Explains a single param and returns its name, doc, and optional\n",
      " |      default value and user-supplied value in a string.\n",
      " |  \n",
      " |  explainParams(self)\n",
      " |      Returns the documentation of all params with their optionally\n",
      " |      default values and user-supplied values.\n",
      " |  \n",
      " |  extractParamMap(self, extra=None)\n",
      " |      Extracts the embedded default param values and user-supplied\n",
      " |      values, and then merges them with extra values from input into\n",
      " |      a flat param map, where the latter value is used if there exist\n",
      " |      conflicts, i.e., with ordering: default param values <\n",
      " |      user-supplied values < extra.\n",
      " |      \n",
      " |      :param extra: extra param values\n",
      " |      :return: merged param map\n",
      " |  \n",
      " |  getOrDefault(self, param)\n",
      " |      Gets the value of a param in the user-supplied param map or its\n",
      " |      default value. Raises an error if neither is set.\n",
      " |  \n",
      " |  getParam(self, paramName)\n",
      " |      Gets a param by its name.\n",
      " |  \n",
      " |  hasDefault(self, param)\n",
      " |      Checks whether a param has a default value.\n",
      " |  \n",
      " |  hasParam(self, paramName)\n",
      " |      Tests whether this instance contains a param with a given\n",
      " |      (string) name.\n",
      " |  \n",
      " |  isDefined(self, param)\n",
      " |      Checks whether a param is explicitly set by user or has\n",
      " |      a default value.\n",
      " |  \n",
      " |  isSet(self, param)\n",
      " |      Checks whether a param is explicitly set by user.\n",
      " |  \n",
      " |  set(self, param, value)\n",
      " |      Sets a parameter in the embedded param map.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  params\n",
      " |      Returns all params ordered by name. The default implementation\n",
      " |      uses :py:func:`dir` to get all attributes of type\n",
      " |      :py:class:`Param`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.Identifiable:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n",
      " |  \n",
      " |  read() from builtins.type\n",
      " |      Returns an MLReader instance for this class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.MLReadable:\n",
      " |  \n",
      " |  load(path) from builtins.type\n",
      " |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n",
      " |  \n",
      " |  write(self)\n",
      " |      Returns an MLWriter instance for this ML instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.MLWritable:\n",
      " |  \n",
      " |  save(self, path)\n",
      " |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "weekly-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "del cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-blackjack",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
