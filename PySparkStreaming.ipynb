{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fewer-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    print(\"no sc to stop\")\n",
    "    \n",
    "try:\n",
    "    ssc.stop()\n",
    "except:\n",
    "    print(\"no ssc to stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fitted-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lonely-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName(\"StructuredNetworkWordCount\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "sc = SparkContext(\"local[2]\", \"SparkStreamingWordCount\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "speaking-thinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc=StreamingContext(sc,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "determined-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame representing the stream of input lines from connection to localhost:9999\n",
    "# lines = spark \\\n",
    "#     .readStream \\\n",
    "#     .format(\"socket\") \\\n",
    "#     .option(\"host\", \"localhost\") \\\n",
    "#     .option(\"port\", 9999) \\\n",
    "#     .load()\n",
    "lines = ssc.socketTextStream(\"localhost\", 9990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "failing-berkeley",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "# Generate running word count\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y : x+1)\n",
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "grateful-contractor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DStream in module pyspark.streaming.dstream object:\n",
      "\n",
      "class DStream(builtins.object)\n",
      " |  A Discretized Stream (DStream), the basic abstraction in Spark Streaming,\n",
      " |  is a continuous sequence of RDDs (of the same type) representing a\n",
      " |  continuous stream of data (see L{RDD} in the Spark core documentation\n",
      " |  for more details on RDDs).\n",
      " |  \n",
      " |  DStreams can either be created from live data (such as, data from TCP\n",
      " |  sockets, Kafka, Flume, etc.) using a L{StreamingContext} or it can be\n",
      " |  generated by transforming existing DStreams using operations such as\n",
      " |  `map`, `window` and `reduceByKeyAndWindow`. While a Spark Streaming\n",
      " |  program is running, each DStream periodically generates a RDD, either\n",
      " |  from live data or by transforming the RDD generated by a parent DStream.\n",
      " |  \n",
      " |  DStreams internally is characterized by a few basic properties:\n",
      " |   - A list of other DStreams that the DStream depends on\n",
      " |   - A time interval at which the DStream generates an RDD\n",
      " |   - A function that is used to generate an RDD after each time interval\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, jdstream, ssc, jrdd_deserializer)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  cache(self)\n",
      " |      Persist the RDDs of this DStream with the default storage level\n",
      " |      (C{MEMORY_ONLY}).\n",
      " |  \n",
      " |  checkpoint(self, interval)\n",
      " |      Enable periodic checkpointing of RDDs of this DStream\n",
      " |      \n",
      " |      @param interval: time in seconds, after each period of that, generated\n",
      " |                       RDD will be checkpointed\n",
      " |  \n",
      " |  cogroup(self, other, numPartitions=None)\n",
      " |      Return a new DStream by applying 'cogroup' between RDDs of this\n",
      " |      DStream and `other` DStream.\n",
      " |      \n",
      " |      Hash partitioning is used to generate the RDDs with `numPartitions` partitions.\n",
      " |  \n",
      " |  combineByKey(self, createCombiner, mergeValue, mergeCombiners, numPartitions=None)\n",
      " |      Return a new DStream by applying combineByKey to each RDD.\n",
      " |  \n",
      " |  context(self)\n",
      " |      Return the StreamingContext associated with this DStream\n",
      " |  \n",
      " |  count(self)\n",
      " |      Return a new DStream in which each RDD has a single element\n",
      " |      generated by counting each RDD of this DStream.\n",
      " |  \n",
      " |  countByValue(self)\n",
      " |      Return a new DStream in which each RDD contains the counts of each\n",
      " |      distinct value in each RDD of this DStream.\n",
      " |  \n",
      " |  countByValueAndWindow(self, windowDuration, slideDuration, numPartitions=None)\n",
      " |      Return a new DStream in which each RDD contains the count of distinct elements in\n",
      " |      RDDs in a sliding window over this DStream.\n",
      " |      \n",
      " |      @param windowDuration: width of the window; must be a multiple of this DStream's\n",
      " |                            batching interval\n",
      " |      @param slideDuration:  sliding interval of the window (i.e., the interval after which\n",
      " |                            the new DStream will generate RDDs); must be a multiple of this\n",
      " |                            DStream's batching interval\n",
      " |      @param numPartitions:  number of partitions of each RDD in the new DStream.\n",
      " |  \n",
      " |  countByWindow(self, windowDuration, slideDuration)\n",
      " |      Return a new DStream in which each RDD has a single element generated\n",
      " |      by counting the number of elements in a window over this DStream.\n",
      " |      windowDuration and slideDuration are as defined in the window() operation.\n",
      " |      \n",
      " |      This is equivalent to window(windowDuration, slideDuration).count(),\n",
      " |      but will be more efficient if window is large.\n",
      " |  \n",
      " |  filter(self, f)\n",
      " |      Return a new DStream containing only the elements that satisfy predicate.\n",
      " |  \n",
      " |  flatMap(self, f, preservesPartitioning=False)\n",
      " |      Return a new DStream by applying a function to all elements of\n",
      " |      this DStream, and then flattening the results\n",
      " |  \n",
      " |  flatMapValues(self, f)\n",
      " |      Return a new DStream by applying a flatmap function to the value\n",
      " |      of each key-value pairs in this DStream without changing the key.\n",
      " |  \n",
      " |  foreachRDD(self, func)\n",
      " |      Apply a function to each RDD in this DStream.\n",
      " |  \n",
      " |  fullOuterJoin(self, other, numPartitions=None)\n",
      " |      Return a new DStream by applying 'full outer join' between RDDs of this DStream and\n",
      " |      `other` DStream.\n",
      " |      \n",
      " |      Hash partitioning is used to generate the RDDs with `numPartitions`\n",
      " |      partitions.\n",
      " |  \n",
      " |  glom(self)\n",
      " |      Return a new DStream in which RDD is generated by applying glom()\n",
      " |      to RDD of this DStream.\n",
      " |  \n",
      " |  groupByKey(self, numPartitions=None)\n",
      " |      Return a new DStream by applying groupByKey on each RDD.\n",
      " |  \n",
      " |  groupByKeyAndWindow(self, windowDuration, slideDuration, numPartitions=None)\n",
      " |      Return a new DStream by applying `groupByKey` over a sliding window.\n",
      " |      Similar to `DStream.groupByKey()`, but applies it over a sliding window.\n",
      " |      \n",
      " |      @param windowDuration: width of the window; must be a multiple of this DStream's\n",
      " |                            batching interval\n",
      " |      @param slideDuration:  sliding interval of the window (i.e., the interval after which\n",
      " |                            the new DStream will generate RDDs); must be a multiple of this\n",
      " |                            DStream's batching interval\n",
      " |      @param numPartitions:  Number of partitions of each RDD in the new DStream.\n",
      " |  \n",
      " |  join(self, other, numPartitions=None)\n",
      " |      Return a new DStream by applying 'join' between RDDs of this DStream and\n",
      " |      `other` DStream.\n",
      " |      \n",
      " |      Hash partitioning is used to generate the RDDs with `numPartitions`\n",
      " |      partitions.\n",
      " |  \n",
      " |  leftOuterJoin(self, other, numPartitions=None)\n",
      " |      Return a new DStream by applying 'left outer join' between RDDs of this DStream and\n",
      " |      `other` DStream.\n",
      " |      \n",
      " |      Hash partitioning is used to generate the RDDs with `numPartitions`\n",
      " |      partitions.\n",
      " |  \n",
      " |  map(self, f, preservesPartitioning=False)\n",
      " |      Return a new DStream by applying a function to each element of DStream.\n",
      " |  \n",
      " |  mapPartitions(self, f, preservesPartitioning=False)\n",
      " |      Return a new DStream in which each RDD is generated by applying\n",
      " |      mapPartitions() to each RDDs of this DStream.\n",
      " |  \n",
      " |  mapPartitionsWithIndex(self, f, preservesPartitioning=False)\n",
      " |      Return a new DStream in which each RDD is generated by applying\n",
      " |      mapPartitionsWithIndex() to each RDDs of this DStream.\n",
      " |  \n",
      " |  mapValues(self, f)\n",
      " |      Return a new DStream by applying a map function to the value of\n",
      " |      each key-value pairs in this DStream without changing the key.\n",
      " |  \n",
      " |  partitionBy(self, numPartitions, partitionFunc=<function portable_hash at 0x7f5e2c5a5268>)\n",
      " |      Return a copy of the DStream in which each RDD are partitioned\n",
      " |      using the specified partitioner.\n",
      " |  \n",
      " |  persist(self, storageLevel)\n",
      " |      Persist the RDDs of this DStream with the given storage level\n",
      " |  \n",
      " |  pprint(self, num=10)\n",
      " |      Print the first num elements of each RDD generated in this DStream.\n",
      " |      \n",
      " |      @param num: the number of elements from the first will be printed.\n",
      " |  \n",
      " |  reduce(self, func)\n",
      " |      Return a new DStream in which each RDD has a single element\n",
      " |      generated by reducing each RDD of this DStream.\n",
      " |  \n",
      " |  reduceByKey(self, func, numPartitions=None)\n",
      " |      Return a new DStream by applying reduceByKey to each RDD.\n",
      " |  \n",
      " |  reduceByKeyAndWindow(self, func, invFunc, windowDuration, slideDuration=None, numPartitions=None, filterFunc=None)\n",
      " |      Return a new DStream by applying incremental `reduceByKey` over a sliding window.\n",
      " |      \n",
      " |      The reduced value of over a new window is calculated using the old window's reduce value :\n",
      " |       1. reduce the new values that entered the window (e.g., adding new counts)\n",
      " |       2. \"inverse reduce\" the old values that left the window (e.g., subtracting old counts)\n",
      " |      \n",
      " |      `invFunc` can be None, then it will reduce all the RDDs in window, could be slower\n",
      " |      than having `invFunc`.\n",
      " |      \n",
      " |      @param func:           associative and commutative reduce function\n",
      " |      @param invFunc:        inverse function of `reduceFunc`\n",
      " |      @param windowDuration: width of the window; must be a multiple of this DStream's\n",
      " |                            batching interval\n",
      " |      @param slideDuration:  sliding interval of the window (i.e., the interval after which\n",
      " |                            the new DStream will generate RDDs); must be a multiple of this\n",
      " |                            DStream's batching interval\n",
      " |      @param numPartitions:  number of partitions of each RDD in the new DStream.\n",
      " |      @param filterFunc:     function to filter expired key-value pairs;\n",
      " |                            only pairs that satisfy the function are retained\n",
      " |                            set this to null if you do not want to filter\n",
      " |  \n",
      " |  reduceByWindow(self, reduceFunc, invReduceFunc, windowDuration, slideDuration)\n",
      " |      Return a new DStream in which each RDD has a single element generated by reducing all\n",
      " |      elements in a sliding window over this DStream.\n",
      " |      \n",
      " |      if `invReduceFunc` is not None, the reduction is done incrementally\n",
      " |      using the old window's reduced value :\n",
      " |      \n",
      " |      1. reduce the new values that entered the window (e.g., adding new counts)\n",
      " |      \n",
      " |      2. \"inverse reduce\" the old values that left the window (e.g., subtracting old counts)\n",
      " |      This is more efficient than `invReduceFunc` is None.\n",
      " |      \n",
      " |      @param reduceFunc:     associative and commutative reduce function\n",
      " |      @param invReduceFunc:  inverse reduce function of `reduceFunc`; such that for all y,\n",
      " |                             and invertible x:\n",
      " |                             `invReduceFunc(reduceFunc(x, y), x) = y`\n",
      " |      @param windowDuration: width of the window; must be a multiple of this DStream's\n",
      " |                             batching interval\n",
      " |      @param slideDuration:  sliding interval of the window (i.e., the interval after which\n",
      " |                             the new DStream will generate RDDs); must be a multiple of this\n",
      " |                             DStream's batching interval\n",
      " |  \n",
      " |  repartition(self, numPartitions)\n",
      " |      Return a new DStream with an increased or decreased level of parallelism.\n",
      " |  \n",
      " |  rightOuterJoin(self, other, numPartitions=None)\n",
      " |      Return a new DStream by applying 'right outer join' between RDDs of this DStream and\n",
      " |      `other` DStream.\n",
      " |      \n",
      " |      Hash partitioning is used to generate the RDDs with `numPartitions`\n",
      " |      partitions.\n",
      " |  \n",
      " |  saveAsTextFiles(self, prefix, suffix=None)\n",
      " |      Save each RDD in this DStream as at text file, using string\n",
      " |      representation of elements.\n",
      " |  \n",
      " |  slice(self, begin, end)\n",
      " |      Return all the RDDs between 'begin' to 'end' (both included)\n",
      " |      \n",
      " |      `begin`, `end` could be datetime.datetime() or unix_timestamp\n",
      " |  \n",
      " |  transform(self, func)\n",
      " |      Return a new DStream in which each RDD is generated by applying a function\n",
      " |      on each RDD of this DStream.\n",
      " |      \n",
      " |      `func` can have one argument of `rdd`, or have two arguments of\n",
      " |      (`time`, `rdd`)\n",
      " |  \n",
      " |  transformWith(self, func, other, keepSerializer=False)\n",
      " |      Return a new DStream in which each RDD is generated by applying a function\n",
      " |      on each RDD of this DStream and 'other' DStream.\n",
      " |      \n",
      " |      `func` can have two arguments of (`rdd_a`, `rdd_b`) or have three\n",
      " |      arguments of (`time`, `rdd_a`, `rdd_b`)\n",
      " |  \n",
      " |  union(self, other)\n",
      " |      Return a new DStream by unifying data of another DStream with this DStream.\n",
      " |      \n",
      " |      @param other: Another DStream having the same interval (i.e., slideDuration)\n",
      " |                   as this DStream.\n",
      " |  \n",
      " |  updateStateByKey(self, updateFunc, numPartitions=None, initialRDD=None)\n",
      " |      Return a new \"state\" DStream where the state for each key is updated by applying\n",
      " |      the given function on the previous state of the key and the new values of the key.\n",
      " |      \n",
      " |      @param updateFunc: State update function. If this function returns None, then\n",
      " |                         corresponding state key-value pair will be eliminated.\n",
      " |  \n",
      " |  window(self, windowDuration, slideDuration=None)\n",
      " |      Return a new DStream in which each RDD contains all the elements in seen in a\n",
      " |      sliding window of time over this DStream.\n",
      " |      \n",
      " |      @param windowDuration: width of the window; must be a multiple of this DStream's\n",
      " |                            batching interval\n",
      " |      @param slideDuration:  sliding interval of the window (i.e., the interval after which\n",
      " |                            the new DStream will generate RDDs); must be a multiple of this\n",
      " |                            DStream's batching interval\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "solved-status",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-02-02 23:35:30\n",
      "-------------------------------------------\n",
      "('4', 2)\n",
      "('', 1)\n",
      "('1', 2)\n",
      "('3', 2)\n",
      "('2', 2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-02-02 23:35:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-02-02 23:35:40\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-18f3db416f1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/streaming/context.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \"\"\"\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-binding",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-henry",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-barbados",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
