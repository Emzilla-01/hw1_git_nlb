{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "threatened-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "suited-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType, DateType\n",
    "\n",
    "\n",
    "conf = SparkConf().setAppName('Spark-logAnalysis').setMaster('local[4]')\n",
    "sc = SparkContext.getOrCreate(conf = conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-timer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ancient-willow",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder.appName('sparklog').getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "simple-doubt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark-logAnalysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8e019f6828>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "tender-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"file:///home/hadoop/data/access_log_Jul95\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "varying-theory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1891715"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "honey-iraqi",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245',\n",
       " 'unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985',\n",
       " '199.120.110.21 - - [01/Jul/1995:00:00:09 -0400] \"GET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0\" 200 4085',\n",
       " 'burger.letters.com - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/countdown/liftoff.html HTTP/1.0\" 304 0',\n",
       " '199.120.110.21 - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/missions/sts-73/sts-73-patch-small.gif HTTP/1.0\" 200 4179']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "optimum-channels",
   "metadata": {},
   "outputs": [],
   "source": [
    "#expr=r'^(\\S+)((\\s)(-))+\\s(\\[\\S+ -\\d{4}\\])\\s(\"\\w+\\s+([^\\s]+)\\s+HTTP.*\")\\s(\\d{3}\\s(\\d*)$)'\n",
    "expr=r\"((\\d+.\\d+.\\d+.\\d+)|[\\w\\.]*)\\s[-]\\s[-]\\s[[](\\d{2}[/]\\w{3}[/]\\d{4}[:]\\d{2}[:]\\d{2}[:]\\d{2})\\s([-]\\d{4})[]]\\s([\\\"].*[\\\"])\\s(\\d*)\\s([\\-]|\\d*|)[\\n]*\"\n",
    "\n",
    "\n",
    "def log_expression(line):\n",
    "    # Tried to include match object but got \"TypeError: can't pickle _sre.SRE_Match objects\" inside  result_rdd.take(1)\n",
    "    match = re.search(expr,line)\n",
    "    if match is None:    \n",
    "        return(line,0)\n",
    "    else:\n",
    "        return(line,1)\n",
    "\n",
    "\n",
    "def match_log(line):\n",
    "    match = re.search(expr,line)\n",
    "    return(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = match_log(rdd.take(5)[4])\n",
    "\n",
    "#dir(x)\n",
    "#display(x.groups())\n",
    "\n",
    "keys_dict = {0: 'hostname',\n",
    "             2: 'datetime_s',\n",
    "             3: 'tz',\n",
    "             4: 'request',\n",
    "             5: 'response',\n",
    "             6: 'size'}\n",
    "\n",
    "# [(it[0], it[1], x.groups()[it[0]]) for it in keys_dict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-relationship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_logs = rdd.count()\n",
    "# failed = rdd.map(lambda line: log_expression(line)).filter(lambda line: line[1] == 0).count()\n",
    "# print('Out of a total of {} logs, {} failed to parse'.format(n_logs,failed))\n",
    "# # Get the failed records line[1] == 0\n",
    "# failed1=rdd.map(lambda line: log_expression(line)).filter(lambda line: line[1]==0)\n",
    "# print(\"failed lines:\")\n",
    "# print(failed1.take(3))\n",
    "# print(\"*\"*20)\n",
    "\n",
    "# success = rdd.map(lambda line: log_expression(line).filter(lambda line: line[1]==1))\n",
    "# n_logs = rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-botswana",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_logs = rdd.count()\n",
    "\n",
    "parsed = rdd.map(lambda line: log_expression(line))\n",
    "failed = parsed.filter(lambda line: line[1] == 0)\n",
    "success = parsed.filter(lambda line: line[1] == 1)\n",
    "del parsed\n",
    "print('Out of a total of {} logs, {} failed to parse'.format(n_logs,failed.count()))\n",
    "print(\"failed lines:\")\n",
    "print(failed.take(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-gilbert",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(type(success),\n",
    "dir(success))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-management",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "success.take(1)\n",
    "# hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-denver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #https://sparkbyexamples.com/pyspark/pyspark-create-an-empty-dataframe/\n",
    "# schema = StructType([\n",
    "#     StructField('hostname', StringType(), True), \n",
    "#     StructField('datetime_s', StringType(), True), \n",
    "#     StructField('tz', StringType(), True), \n",
    "#     StructField('request', StringType(), True), \n",
    "#     StructField('response', StringType(), True), \n",
    "#     StructField('size', StringType(), True)])\n",
    "\n",
    "# # emptyrdd= sc.parallelize([])\n",
    "\n",
    "# # df = spark.createDataFrame(emptyrdd, schema)\n",
    "# # df = sc.parallelize([]).toDF(schema)\n",
    "# # df = spark.createDataFrame([],schema)\n",
    "\n",
    "# df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-rouge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-silicon",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keys_dict)\n",
    "#date format #https://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-dancing",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = SQLContext.createDataFrame(sc.emptyRDD(), schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-prince",
   "metadata": {},
   "outputs": [],
   "source": [
    "issubclass(StringType, DataType), isinstance(StringType, DataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(DataType, DataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "requested-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logs_df = spark.read.text('file:///home/hadoop/data/access_log_Jul95')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "suspended-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile_df = logs_df\n",
    "sample_logs = logs_df.sample(withReplacement=None, fraction=.01, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "molecular-hebrew",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'Rows'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-34c31cc8f802>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msample_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msample_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1305\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1306\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'Rows'"
     ]
    }
   ],
   "source": [
    "sample_logs.take(5)\n",
    "sample_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "experimental-evidence",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-2b755efd5283>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0;34m'no match'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         for item in sample_logs]\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mts_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'\\[(\\d{2}/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-2b755efd5283>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0;34m'no match'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         for item in sample_logs]\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mts_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'\\[(\\d{2}/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/re.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \"\"\"Scan through string looking for a match to the pattern, returning\n\u001b[1;32m    172\u001b[0m     a match object, or None if no match was found.\"\"\"\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "\n",
    "import re \n",
    "host_pattern = r'(^\\S+\\.[\\S+\\.]+\\S+)\\s'\n",
    "\n",
    "hosts = [re.search(host_pattern, item).group(1)\n",
    "        if re.search(host_pattern, item)\n",
    "        else 'no match'\n",
    "        for item in sample_logs]\n",
    "\n",
    "ts_pattern = r'\\[(\\d{2}/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})]'\n",
    "timestamps = [re.search(ts_pattern, item).group(1) for item in sample_logs]\n",
    "timestamps\n",
    "\n",
    "method_uri_pattern = r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"'\n",
    "method_uri_protocol = [re.search(method_uri_pattern, item).groups() \n",
    "                       if re.search(method_uri_pattern, item)\n",
    "                       else 'no match'\n",
    "                       for item in sample_logs]\n",
    "method_uri_protocol\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "logfile_df = logs_df.select(regexp_extract('value', host_pattern, 1).alias('host'), \n",
    "                     regexp_extract('value', ts_pattern, 1).alias('timestamp'),\n",
    "                     regexp_extract('value', method_uri_pattern, 1).alias('method'),\n",
    "                     regexp_extract('value', method_uri_pattern, 2).alias('endpoint'),\n",
    "                     regexp_extract('value', method_uri_pattern, 3).alias('protocol'), \n",
    "                     regexp_extract('value', status_pattern, 1).alias('status'), \n",
    "                     regexp_extract('value', content_size_pattern, 1).cast('integer').alias('content_size'))\n",
    "logfile_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "expired-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "def count_na(col_name):\n",
    "    return spark_sum(col (col_name).isNull().cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "spatial-google",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "def count_na(col_name):\n",
    "    return spark_sum(col(col_name).isNull().cast('integer')).alias(col_name)\n",
    "\n",
    "# Build a list of column expressions , one per column\n",
    "exprs = [count_na(col_name) for col_name in logfile_df.columns]\n",
    "\n",
    "# *exprs converts the list of expression into variable function arguments i.e. unpacks the list\n",
    "logfile_df.agg(*exprs).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "numeric-functionality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logfile_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD - persistance & cache\n",
    "# cache - IN Memory\n",
    "# pesist - DISK_ONLY, MEMORY_AND_DISK, MEMORY_ONLY\n",
    "logfile_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-joint",
   "metadata": {},
   "source": [
    "#### EDA & Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "regional-parish",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`content_size`' given input columns: [value];;\\n'Project ['content_size]\\n+- Relation[value#0] text\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o276.describe.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`content_size`' given input columns: [value];;\n'Project ['content_size]\n+- Relation[value#0] text\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1358)\n\tat org.apache.spark.sql.Dataset.describe(Dataset.scala:2477)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-e738fa8293a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogfile_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mdescribe\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`content_size`' given input columns: [value];;\\n'Project ['content_size]\\n+- Relation[value#0] text\\n\""
     ]
    }
   ],
   "source": [
    "logfile_df.describe(['content_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "reserved-uruguay",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, value: string]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logfile_df.describe('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-ecuador",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logfile_df.describe(['content_size']).toPandas()\n",
    "logfile_df.describe(['content_size']).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-physiology",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_code_df = (logfile_df.groupby('status').count(),sort('status').cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-maryland",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_code_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-bishop",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_code_df.toPandas().sort_values(by=['count'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-toronto",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_code_final = status_code_df.toPandas().sort_values(by = ['count'], ascending=False)\n",
    "status_code_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "matched-sport",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-d83da762fb73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='status', y='count', data=status_code_final, kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import log\n",
    "\n",
    "log_freq_df = status_code_df.withColumn('log(count)', log(status_code_df['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "numeric-botswana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function log in module pyspark.sql.functions:\n",
      "\n",
      "log(arg1, arg2=None)\n",
      "    Returns the first argument-based logarithm of the second argument.\n",
      "    \n",
      "    If there is only one argument, then this takes the natural logarithm of the argument.\n",
      "    \n",
      "    >>> df.select(log(10.0, df.age).alias('ten')).rdd.map(lambda l: str(l.ten)[:7]).collect()\n",
      "    ['0.30102', '0.69897']\n",
      "    \n",
      "    >>> df.select(log(df.age).alias('e')).rdd.map(lambda l: str(l.e)[:7]).collect()\n",
      "    ['0.69314', '1.60943']\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import log\n",
    "help(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_freq_final = log_freq_df.toPandas().sort_values(by=['log(count)'], ascending=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
